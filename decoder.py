from attention import MaskedMultiHeadedSelfAttention
import torch

class DecoderLayer(torch.nn.Module):
    """
    Pytorch module for an encoder layer.An encoder layer consists of a multi-headed self attention layer, a feed forward layer and
    dropout.
    Input dimension is: (batch_size, sequence_length, embedding_dimension)
    Output dimension is: (batch_size, sequence_length, embedding_dimension)
    """
    def __init__(
    self,
    embedding_dimension,
    number_of_heads,
    feed_forward_dimension,
    dropout_rate
    ):
        super().__init__()
        self.embedding_dimension = embedding_dimension
        self.number_of_heads = number_of_heads
        self.feed_forward_dimension = feed_forward_dimension
        self.dropout_rate = dropout_rate
        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)
        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)
        self.dropout = torch.nn.Dropout(dropout_rate)
        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)
        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)
    def forward(self, x, mask):
        """
        Compute the encoder layer.
        x dimensions are: (batch_size, sequence_length, embedding_dimension)
        mask dimensions are: (batch_size, sequence_length)
        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.
        """
        # Layer normalization 1
        normalized_x = self.layer_normalization_1(x)
        # Multi headed self attention
        attention_output = self.multi_headed_self_attention(normalized_x, mask)
        # Residual output
        residual_output = x + attention_output# Layer normalization 2
        normalized_residual_output = self.layer_normalization_2(residual_output)
        # Feed forward
        feed_forward_output = self.feed_forward(normalized_residual_output)
        # Dropout, only when training.
        if self.training:
            feed_forward_output = self.dropout(feed_forward_output)
        # Residual output
        return residual_output + feed_forward_output
    

class DecoderStack(torch.nn.Module):
    """
    Pytorch module for a stack of decoders.
    """
    def __init__(
    self,
    embedding_dimension,
    number_of_layers,
    number_of_heads,
    feed_forward_dimension,
    dropout_rate,
    max_sequence_length
    ):
        
        super().__init__()
        self.embedding_dimension = embedding_dimension
        self.number_of_layers = number_of_layers
        self.number_of_heads = number_of_heads
        self.feed_forward_dimension = feed_forward_dimension
        self.dropout_rate = dropout_rate
        self.max_sequence_length = max_sequence_length
        # Create the encoder layers
        self.encoder_layers = torch.nn.ModuleList(
            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension,dropout_rate) for _ in range(number_of_layers)])
    
    def forward(self, x, mask):
        decoder_outputs = x
        for decoder_layer in self.encoder_layers:
            decoder_outputs = decoder_layer(decoder_outputs, mask)
        
        return decoder_outputs
    


class FeedForward(torch.nn.Module):
    """
    Pytorch module for a feed forward layer.
    A feed forward layer is a fully connected layer with a ReLU activation function in between.
    """
    def __init__(self, embedding_dimension, feed_forward_dimension):
        super().__init__()
        self.embedding_dimension = embedding_dimension
        self.feed_forward_dimension = feed_forward_dimension
        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)
        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)
   
    def forward(self, x):
        """
        Compute the feed forward layer.
        """
        return self.linear_2(torch.relu(self.linear_1(x)))