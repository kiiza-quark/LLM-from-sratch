{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "from language_model import AutoregressiveWrapper, LanguageModel\n",
    "# from hellodata import tokenized_training_data\n",
    "from generator import Generator\n",
    "from trainer import Trainer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "    # Prepend padding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "    return tokenized_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "embedding_dimension = 256\n",
    "max_sequence_length = 20\n",
    "number_of_tokens = tokenizer.size()\n",
    "# Create the model\n",
    "model = AutoregressiveWrapper(LanguageModel(\n",
    "embedding_dimension=embedding_dimension,\n",
    "number_of_tokens=number_of_tokens,\n",
    "number_of_heads=4,\n",
    "number_of_layers=3,\n",
    "dropout_rate=0.1,\n",
    "max_sequence_length=max_sequence_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Loss: 0.6644536852836609\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.6644536852836609]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the training data\n",
    "training_data = '. '.join([\n",
    "'cats rule the world',\n",
    "'dogs are the best','elephants have long trunks',\n",
    "'monkeys like bananas',\n",
    "'pandas eat bamboo',\n",
    "'tigers are dangerous',\n",
    "'zebras have stripes',\n",
    "'lions are the kings of the savannah',\n",
    "'giraffes have long necks',\n",
    "'hippos are big and scary',\n",
    "'rhinos have horns',\n",
    "'penguins live in the arctic',\n",
    "'polar bears are white'\n",
    "])\n",
    "\n",
    "with open(\"makedata/data.txt\", 'r') as file:\n",
    "    training_data = '. '.join([line.lower() for line in file.readlines()])\n",
    "     \n",
    "tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
    "sequences = create_training_sequences(max_sequence_length,\n",
    "tokenized_and_padded_training_data)\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "trainer = Trainer(model, tokenizer, optimizer)\n",
    "trainer.train(sequences, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wingedsheep: Artificial Intelligence Blog\\n. Building a text generation model from scratch\\n. VINCENT BONS - 20 NOV 2022\\n. Building a text generation model from scratch\\n. The transformer is a neural network architecture that was proposed in the paper \"Attention is All\\n. You Need\" by Ashish Vaswani, et al. in 2017. It is a powerful architecture that lead to many\\n. state-of-the-art results in the last few years. It can be used to generate text (GPT-3), create\\n. beautiful images from text (Imagen, Dall-e 2, Stable diffusion), compose music (music\\n. transformer), speech-to-text (Whisper), understanding protein structure (AlphaFold), teaching\\n. cars to drive themselves (Tesla FSD) or even learn to do many different tasks (Gato). There are\\n. probably many amazing results I forgot to name, and many more to come.\\n. Large language models (LLMs) are going to have a large impact in the future. We are probably\\n. going to interact with these models a lot in the future to brainstorm, help us with creativity, make\\n. sense of large amounts of information and maybe even understand ourselves better. GPT-3 is\\n. already helping me with coding and writing (also for this blogpost), and these models are going\\n. to get better really fast. That is why I want to understand how they work.\\n. In this blogpost I will show you how to build a text generation model from scratch using the\\n. transformer architecture. I will show the coding process, and will try to make each step as\\n. simple as possible. The aim of this post is not to build the best text generation model, but to try\\n. to make each step of building one as clear as possible.\\n. Overview\\n. For translation models a combination of encoder and decoder layers is used. The encoder\\n. reads the input sentence and the decoder generates the output sentence. For text generation\\n. models we don\\'t need the encoder, because the input and output sentence are the same. We\\n. can just use the decoder to generate text.\\n. The input of the model is a sequence of integers, representing the text generated so far. Each\\n. integer corresponds to a token in a vocabulary. A token can be a character, a word or anything\\n. you might want to represent.\\n. The output of the model is a probability distribution over the next token. We can sample from\\n. this distribution to get the next token.\\n. This is an overview of the model. In the next sections I will zoom in on all parts of this diagram.\\n. Tokenizing the text\\n. We want to train our model on a text dataset. But before we can use the text in our model it has\\n. to be tokenized.Tokenizing a text means converting it into a sequence of tokens. A token is an atomic unit of\\n. text. A token can be a character, a word or anything you might want to represent. GPT-3 uses\\n. combinations of characters that frequently occur together as tokens.\\n. There are different ways to tokenize a text. The most common way is to use a vocabulary. A\\n. vocabulary is a set of all possible tokens. Each token in the text is represented by an integer,\\n. called an index. This integer is the index of the token in the vocabulary.\\n. For example, if our text is \"I am a cat\" and our vocabulary is [\"I\", \"am\", \"a\", \"cat\", \"dog\", \"mouse\"]\\n. then our text will be represented as [1, 2, 3, 4, 0, 0]. The 0\\'s are called padding. Padding is used\\n. to make all texts the same length.\\n. We create a very simple tokenizer that can encode the characters a-z, the numbers 0-9 a period\\n. and a whitespace character. The padding token is mapped to 0.\\n. Our dictionary is very simple, and contains only 39 tokens. For comparison, the dictionary that\\n. OpenAI uses for GPT-3 contains 50257 tokens.\\n. class Tokenizer:\\n. def __init__(self):\\n. self.dictionary = {}\\n. self.reverse_dictionary = {}\\n. # Add the padding token\\n. self.__add_to_dict(\\'<pad>\\')\\n. # Add characters and numbers to the dictionary\\n. for i in range(10):\\n. self.__add_to_dict(str(i))\\n. for i in range(26):\\n. self.__add_to_dict(chr(ord(\\'a\\') + i))\\n. # Add space and punctuation to the dictionary\\n. self.__add_to_dict(\\'.\\')\\n. self.__add_to_dict(\\' \\')\\n. def __add_to_dict(self, character):\\n. if character not in self.dictionary:\\n. self.dictionary[character] = len(self.dictionary)\\n. self.reverse_dictionary[self.dictionary[character]] = character\\n. def tokenize(self, text):\\n. return [self.dictionary[c] for c in text]def character_to_token(self, character):\\n. return self.dictionary[character]\\n. def token_to_character(self, token):\\n. return self.reverse_dictionary[token]\\n. def size(self):\\n. return len(self.dictionary)\\n. Dataset\\n. Then we have to define a dataset to train on. I am going to define some sort of \"hello world\"\\n. dataset for text generation.\\n. We create and tokenize the (mini) training dataset as follows. To make sure that all data is taken\\n. into account the training data is padded on the left.\\n. # Create the training data\\n. training_data = \\'. \\'.join([\\n. \\'cats rule the world\\',\\n. \\'dogs are the best\\',\\n. \\'elephants have long trunks\\',\\n. \\'monkeys like bananas\\',\\n. \\'pandas eat bamboo\\',\\n. \\'tigers are dangerous\\',\\n. \\'zebras have stripes\\',\\n. \\'lions are the kings of the savannah\\',\\n. \\'giraffes have long necks\\',\\n. \\'hippos are big and scary\\',\\n. \\'rhinos have horns\\',\\n. \\'penguins live in the arctic\\',\\n. \\'polar bears are white\\'\\n. ])\\n. # Tokenize the training data\\n. tokenized_training_data = tokenizer.tokenize(training_data)\\n. # Add padding to the left, to make sure all parts of the sequence are being trained\\n. for _ in range(max_sequence_length):\\n. # Prepend padding tokens\\n. tokenized_training_data.insert(0, tokenizer.character_to_token(\\'<pad>\\'))\\n. Input embedding\\n. The embedding layer converts tokens to vector representations. The idea is to convert the token\\n. to a vector such that similar tokens are close together in the vector space. If words are used astokens, the model will learn, for example, that cats and dogs are often used in similar contexts,\\n. so they would probably be close in vector space.\\n. The nn.embedding layer performs the embedding step. The weights of the embedding layer are\\n. learned during the training process.\\n. class TokenEmbedding(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module that converts tokens into embeddings.\\n. Input dimension is: (batch_size, sequence_length)\\n. Output dimension is: (batch_size, sequence_length, embedding_dimension)\\n. \"\"\"\\n. def __init__(\\n. self,\\n. embedding_dimension,\\n. number_of_tokens\\n. ):\\n. super().__init__()\\n. self.embedding_layer = torch.nn.Embedding(\\n. num_embeddings=number_of_tokens,\\n. embedding_dim=embedding_dimension\\n. )\\n. def forward(self, x):\\n. return self.embedding_layer(x)\\n. number_of_tokens indicates how many different tokens can be used in the input. This would be\\n. the number of tokens in our dictionary.\\n. embedding_dimension denotes the size of the embedding. A larger embedding means that\\n. more information can be encoded in the vector, but it also means that the model will take longer\\n. to train.\\n. Positional encoding\\n. The positional encoding is used to allow the model to learn about the order of the input tokens.\\n. Every token is processed in parallel, so you could just see the input as a bag of shuffled words.\\n. Without positional encoding there is no way for the model to see the difference between the\\n. sentence \"cats are bigger than mice\" and \"mice are bigger than cats\", since they contain the\\n. same words.\\n. To do this a positional embedding adds a vector to the embedding based on its relative position\\n. in the sentence. This embedding is based on a sinusoidal function.for pos in range(self.max_sequence_length):\\n. for i in range(0, self.embedding_dimension, 2):\\n. P_E(pos,2i) = sin(pos/10000^(2i/embedding_dimension))\\n. P_E(pos,2i+1) = cos(pos/10000^(2i/embedding_dimension))\\n. The code to create a module that creates and adds such a positional embedding could be\\n. written as follows:\\n. class PositionalEncoding(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module that creates a positional embedding with the same dimensions as the token\\n. embeddings.\\n. \"\"\"\\n. def __init__(self, embedding_dimension, max_sequence_length):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.max_sequence_length = max_sequence_length\\n. self.positional_encoding = self.create_positional_encoding()\\n. def create_positional_encoding(self):\\n. \"\"\"\\n. Creates a positional encoding matrix of size (max_sequence_length,\\n. embedding_dimension)\\n. \"\"\"\\n. positional_encoding = np.zeros((self.max_sequence_length, self.embedding_dimension))\\n. for pos in range(self.max_sequence_length):\\n. for i in range(0, self.embedding_dimension, 2):\\n. positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) /\\n. self.embedding_dimension)))\\n. positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) /\\n. self.embedding_dimension)))\\n. return torch.from_numpy(positional_encoding).float()\\n. def forward(self, x):\\n. \"\"\"\\n. Adds the positional encoding to the token embeddings.\\n. \"\"\"\\n. return x + self.positional_encoding[:x.size(0), :]\\n. Attention\\n. To be able to learn the relationships between the tokens in the input, the transformer uses\\n. attention. An attention mechanism can be thought of as a method to focus on parts of the input\\n. that are relevant to each other. It does so by calculating an attention score. For each word, allwords in the sentence are considered. If the other word is important context we want to put\\n. more emphasis on it and it should get a higher attention score.\\n. Say you have the sentence: \"The man ate a sandwich he prepared in the morning. It was\\n. topped with cheese\", and we want to calculate the attention scores for \"It\". \"Sandwich\" should\\n. get a high score, because this is what \"It\" refers to, but \"in\" might receive a lower attention\\n. score. The tokens with the highest attention scores have most influence on the output.\\n. To calculate these attention scores we need a query, key and value vector. These vectors are\\n. created by multiplying the input embedding with a learned matrix (or a linear layer in the neural\\n. network). The query, key and value vectors are then used to calculate the attention scores like\\n. in the diagram below\\n. The attention scores for the tokens are determined by calculating the dot product between the\\n. query and key vectors.\\n. After that the values of the resulting matrix are divided by the square root of the query/key/value\\n. dimension, because according to the paper this leads to more stable gradients.\\n. Optionally we can apply a mask to the attention scores. This is used to prevent the model from\\n. paying attention to the padding. If the mask is 0 the attention score will be -infinity and the\\n. model will not be able to attend to it.\\n. The attention scores are then normalized using the softmax function. After that, the values are\\n. multiplied with the attention scores and summed up. This gives us the output of the attention\\n. layer.\\n. class MaskedSelfAttention(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for a self attention layer.\\n. This layer is used in the MultiHeadedSelfAttention module.\\n. Input dimension is: (batch_size, sequence_length, embedding_dimension)\\n. Output dimension is: (batch_size, sequence_length, head_dimension)\\n. \"\"\"\\n. def __init__(self, embedding_dimension, head_dimension):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.head_dimension = head_dimension\\n. self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\\n. self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\\n. self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\\n. self.softmax = torch.nn.Softmax(dim=-1)def forward(self, x, mask):\\n. \"\"\"\\n. Compute the self attention.\\n. x dimension is: (batch_size, sequence_length, embedding_dimension)\\n. output dimension is: (batch_size, sequence_length, head_dimension)\\n. mask dimension is: (batch_size, sequence_length)\\n. mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\\n. \"\"\"\\n. # x dimensions are: (batch_size, sequence_length, embedding_dimension)\\n. # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\\n. query = self.query_layer(x)\\n. key = self.key_layer(x)\\n. value = self.value_layer(x)\\n. # Calculate the attention weights.\\n. # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\\n. attention_weights = torch.matmul(query, key.transpose(-2, -1))\\n. # Scale the attention weights.\\n. attention_weights = attention_weights / np.sqrt(self.head_dimension)\\n. # Apply the mask to the attention weights, by setting the masked tokens to a very low\\n. value.\\n. # This will make the softmax output 0 for these values.\\n. mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\\n. attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\\n. # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\\n. # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\\n. attention_scores = self.softmax(attention_weights)\\n. # The attention scores are multiplied by the value\\n. # Values of tokens with high attention score get highlighted because they are multiplied by\\n. a larger number,\\n. # and tokens with low attention score get drowned out because they are multiplied by a\\n. smaller number.\\n. # Output dimensions are: (batch_size, sequence_length, head_dimension)\\n. return torch.bmm(attention_scores, value)\\n. Usually multi-headed self attention is used in transformers. A reason for using multiple heads is\\n. that it allows the model to focus on different parts of the input at the same time. Each head canlearn a different representation of the data. This can be helpful for learning tasks that require\\n. understanding of the data from multiple perspectives.\\n. In multi-headed attention there are multiple attention heads that perform the attention step I\\n. previously explained. The attention heads have a linear layer at the end that gives an output\\n. with a certain head dimension. In my example the head dimension is the embedding dimension\\n. divided by the number of heads, but you could choose a different value.\\n. All the outputs of the head dimensions are concatenated into a single matrix. The concatenated\\n. matrix goes through a feed forward layer to give an output with the same dimensions as the\\n. input.\\n. class MaskedMultiHeadedSelfAttention(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for a multi head attention layer.\\n. Input dimension is: (batch_size, sequence_length, embedding_dimension)\\n. Output dimension is: (batch_size, sequence_length, embedding_dimension)\\n. \"\"\"\\n. def __init__(self, embedding_dimension, number_of_heads):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.head_dimension = embedding_dimension // number_of_heads\\n. self.number_of_heads = number_of_heads\\n. # Create the self attention modules\\n. self.self_attentions = torch.nn.ModuleList(\\n. [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in\\n. range(number_of_heads)])\\n. # Create a linear layer to combine the outputs of the self attention modules\\n. self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension,\\n. embedding_dimension)\\n. def forward(self, x, mask):\\n. \"\"\"\\n. Compute the multi head attention.\\n. x dimensions are: (batch_size, sequence_length, embedding_dimension)\\n. mask dimensions are: (batch_size, sequence_length)\\n. mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\\n. \"\"\"\\n. # Compute the self attention for each head# self_attention_outputs dimensions are:\\n. # (number_of_heads, batch_size, sequence_length, head_dimension)\\n. self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\\n. # Concatenate the self attention outputs\\n. # self_attention_outputs_concatenated dimensions are:\\n. # (batch_size, sequence_length, number_of_heads * head_dimension)\\n. concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\\n. # Apply the output layer to the concatenated self attention outputs\\n. # output dimensions are: (batch_size, sequence_length, embedding_dimension)\\n. return self.output_layer(concatenated_self_attention_outputs)\\n. Decoder\\n. The model consists of several decoders. Each decoder takes the output of the previous decoder\\n. as input. The first decoder takes the positional encoding layer as input. The final layer is a\\n. language model head, which is going to output the probabilies of next tokens.\\n. When a decoder receives input from the previous layer it is normalized first. Normalization is\\n. used to make sure that the gradients don\\'t explode. In the normalization step a mean and\\n. variance is calculated for each token. The token is then divided by the square root of the\\n. variance and subtracted by the mean, which means that the mean will be 0 and the variance will\\n. be 1.\\n. After normalization self attention will be applied, like explained in the previous section.\\n. The input is then added to the attention output in a residual step. Residual connections help\\n. mitigate the vanishing gradients problem. The vanishing gradient problem means the gradients\\n. get smaller and smaller when going backwards through the neural network, meaning the\\n. weights in the earlier layers don\\'t get updated as much.\\n. Then the output is normalized again and a feed forward layer is applied. The feed forward layer\\n. is a fully connected layer with a ReLU activation function. In the GPT-2 paper this feed forward\\n. layer has four times the size of the embedding dimension, but this value is not set in stone.\\n. Finally a dropout is applied. In a dropout layer connection are randomly dropped (by default with\\n. a 10% probability in my code) to prevent the model from overfitting to the training data.\\n. The decoder layer in code\\n. class DecoderLayer(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for an encoder layer.An encoder layer consists of a multi-headed self attention layer, a feed forward layer and\\n. dropout.\\n. Input dimension is: (batch_size, sequence_length, embedding_dimension)\\n. Output dimension is: (batch_size, sequence_length, embedding_dimension)\\n. \"\"\"\\n. def __init__(\\n. self,\\n. embedding_dimension,\\n. number_of_heads,\\n. feed_forward_dimension,\\n. dropout_rate\\n. ):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.number_of_heads = number_of_heads\\n. self.feed_forward_dimension = feed_forward_dimension\\n. self.dropout_rate = dropout_rate\\n. self.multi_headed_self_attention =\\n. MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\\n. self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\\n. self.dropout = torch.nn.Dropout(dropout_rate)\\n. self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\\n. self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\\n. def forward(self, x, mask):\\n. \"\"\"\\n. Compute the encoder layer.\\n. x dimensions are: (batch_size, sequence_length, embedding_dimension)\\n. mask dimensions are: (batch_size, sequence_length)\\n. mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\\n. \"\"\"\\n. # Layer normalization 1\\n. normalized_x = self.layer_normalization_1(x)\\n. # Multi headed self attention\\n. attention_output = self.multi_headed_self_attention(normalized_x, mask)\\n. # Residual output\\n. residual_output = x + attention_output# Layer normalization 2\\n. normalized_residual_output = self.layer_normalization_2(residual_output)\\n. # Feed forward\\n. feed_forward_output = self.feed_forward(normalized_residual_output)\\n. # Dropout, only when training.\\n. if self.training:\\n. feed_forward_output = self.dropout(feed_forward_output)\\n. # Residual output\\n. return residual_output + feed_forward_output\\n. The DecoderStack is a number of decoder layers in sequence.\\n. class DecoderStack(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for a stack of decoders.\\n. \"\"\"\\n. def __init__(\\n. self,\\n. embedding_dimension,\\n. number_of_layers,\\n. number_of_heads,\\n. feed_forward_dimension,\\n. dropout_rate,\\n. max_sequence_length\\n. ):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.number_of_layers = number_of_layers\\n. self.number_of_heads = number_of_heads\\n. self.feed_forward_dimension = feed_forward_dimension\\n. self.dropout_rate = dropout_rate\\n. self.max_sequence_length = max_sequence_length\\n. # Create the encoder layers\\n. self.encoder_layers = torch.nn.ModuleList(\\n. [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension,\\n. dropout_rate) for _ in\\n. range(number_of_layers)])\\n. def forward(self, x, mask):decoder_outputs = x\\n. for decoder_layer in self.encoder_layers:\\n. decoder_outputs = decoder_layer(decoder_outputs, mask)\\n. return decoder_outputs\\n. The feed forward layer\\n. class FeedForward(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for a feed forward layer.\\n. A feed forward layer is a fully connected layer with a ReLU activation function in between.\\n. \"\"\"\\n. def __init__(self, embedding_dimension, feed_forward_dimension):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.feed_forward_dimension = feed_forward_dimension\\n. self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\\n. self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\\n. def forward(self, x):\\n. \"\"\"\\n. Compute the feed forward layer.\\n. \"\"\"\\n. return self.linear_2(torch.relu(self.linear_1(x)))\\n. The model\\n. Now we have the embeddings and the decoders we need to bring it all together in an\\n. autoregressive language model.\\n. First we define the LanguageModel class, that brings the different layers together. First the\\n. token embeddings are created and a positional encoding is applied. The output of that is\\n. normalized and goes into the stack of encoders.\\n. Finally we come to the language model head. This is a linear layer that maps the output of the\\n. decoder stack to the number of tokens in the dictionary, so we can compute probabilities for\\n. every token.\\n. class LanguageModel(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for a language model.\\n. \"\"\"def __init__(\\n. self,\\n. number_of_tokens, # The number of tokens in the vocabulary\\n. max_sequence_length=512, # The maximum sequence length to use for attention\\n. embedding_dimension=512, # The dimension of the token embeddings\\n. number_of_layers=6, # The number of decoder layers to use\\n. number_of_heads=4, # The number of attention heads to use\\n. feed_forward_dimension=None, # The dimension of the feed forward layer\\n. dropout_rate=0.1 # The dropout rate to use\\n. ):\\n. super().__init__()\\n. self.number_of_tokens = number_of_tokens\\n. self.max_sequence_length = max_sequence_length\\n. self.embedding_dimension = embedding_dimension\\n. self.number_of_layers = number_of_layers\\n. self.number_of_heads = number_of_heads\\n. if feed_forward_dimension is None:\\n. # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\\n. self.feed_forward_dimension = embedding_dimension * 4\\n. else:\\n. self.feed_forward_dimension = feed_forward_dimension\\n. self.dropout_rate = dropout_rate\\n. # Create the token embedding layer\\n. self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\\n. # Create the positional encoding layer\\n. self.positional_encoding = PositionalEncoding(embedding_dimension,\\n. max_sequence_length)\\n. # Create the normalization layer\\n. self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\\n. # Create the decoder stack\\n. self.decoder = DecoderStack(\\n. embedding_dimension=embedding_dimension,\\n. number_of_layers=number_of_layers,\\n. number_of_heads=number_of_heads,\\n. feed_forward_dimension=self.feed_forward_dimension,\\n. dropout_rate=dropout_rate,\\n. max_sequence_length=max_sequence_length\\n. )# Create the language model head\\n. self.lm_head = LMHead(embedding_dimension, number_of_tokens)\\n. def forward(self, x, mask):\\n. # Compute the token embeddings\\n. # token_embeddings dimensions are: (batch_size, sequence_length,\\n. embedding_dimension)\\n. token_embeddings = self.token_embedding(x)\\n. # Compute the positional encoding\\n. # positional_encoding dimensions are: (batch_size, sequence_length,\\n. embedding_dimension)\\n. positional_encoding = self.positional_encoding(token_embeddings)\\n. # Post embedding layer normalization\\n. positional_encoding_normalized = self.layer_normalization(positional_encoding)\\n. decoder_outputs = self.decoder(positional_encoding_normalized, mask)\\n. lm_head_outputs = self.lm_head(decoder_outputs)\\n. return lm_head_outputs\\n. Code for the language model head.\\n. class LMHead(torch.nn.Module):\\n. \"\"\"\\n. Pytorch module for the language model head.\\n. The language model head is a linear layer that maps the embedding dimension to the\\n. vocabulary size.\\n. \"\"\"\\n. def __init__(self, embedding_dimension, number_of_tokens):\\n. super().__init__()\\n. self.embedding_dimension = embedding_dimension\\n. self.number_of_tokens = number_of_tokens\\n. self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\\n. def forward(self, x):\\n. \"\"\"\\n. Compute the language model head.\\n. x dimensions are: (batch_size, sequence_length, embedding_dimension)\\n. output dimensions are: (batch_size, sequence_length, number_of_tokens)\\n. \"\"\"# Compute the linear layer\\n. # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\\n. linear_output = self.linear(x)\\n. return linear_output\\n. To complete the model we add an autoregressive wrapper (based on the implementation by\\n. lucidrains). Autoregressive means that the output of the previous step is used as input for the\\n. next. We can generate a text by adding one new character at a time this way.\\n. The input of this wrapper is a (batch of) sequence of tokens with a lenght of\\n. max_sequence_length + 1. We add one, because this allows us to shift the target sequence by\\n. one step.\\n. For example if you have the tokens\\n. [\"badgers\", \"are\", \"nocturnal\", \"so\", \"they\", \"sleep\", \"during\", \"the\", \"day\", \"and\", \"are\", \"awake\",\\n. \"at\", \"night\"]\\n. our input would be\\n. [\"badgers\", \"are\", \"nocturnal\", \"so\", \"they\", \"sleep\", \"during\", \"the\", \"day\", \"and\", \"are\", \"awake\",\\n. \"at\"]\\n. and our output would be shifted by one token.\\n. [\"are\", \"nocturnal\", \"so\", \"they\", \"sleep\", \"during\", \"the\", \"day\", \"and\", \"are\", \"awake\", \"at\", \"night\"]\\n. Given the input, we want the model to predict the next token in the sequence, which would in\\n. this case be the word \"night\".\\n. We define a mask based on the padding tokens in the input sequence. Padding tokens are not\\n. going to be attended to.\\n. Then we define a method for this wrapper to calculate the probabilities for the next token. This\\n. method takes an input sequence and predicts the probabilities for the token that comes next,\\n. based on the trained model. It does so by calculating the logits for the last token. The logits are\\n. the output of the neural network before the softmax function is applied. The softmax function is\\n. going to convert these logits into probabilities.\\n. The temperature can be used to control how random the predictions are. If the temperature is 0\\n. the model will only predict the token with the highest probability. The higher the temperature, the\\n. more random the output will be.\\n. class AutoregressiveWrapper(torch.nn.Module):\"\"\"\\n. Pytorch module that wraps a GPT model and makes it autoregressive.\\n. \"\"\"\\n. def __init__(self, gpt_model):\\n. super().__init__()\\n. self.model = gpt_model\\n. self.max_sequence_length = self.model.max_sequence_length\\n. def forward(self, x, mask):\\n. \"\"\"\\n. Autoregressive forward pass\\n. \"\"\"\\n. inp, target = x[:, :-1], x[:, 1:]\\n. mask = mask[:, :-1]\\n. output = self.model(inp, mask)\\n. return output, target\\n. def next_token_probabilities(self, x, mask, temperature=1.0):\\n. \"\"\"\\n. Calculate the token probabilities for the next token in the sequence.\\n. \"\"\"\\n. logits = self.model(x, mask)[:, -1]\\n. # Apply the temperature\\n. if temperature != 1.0:\\n. logits = logits / temperature\\n. # Apply the softmax\\n. probabilities = torch.softmax(logits, dim=-1)\\n. return probabilities\\n. Trainer\\n. Now we have a model that can learn how language works we need to actually train it to do so.\\n. First we create the tokenizer, so we can convert our dataset to tokens.\\n. Then we create the autoregressive language model. Because the dataset is very small I am\\n. going to only set a max_sequence_length of 20, but a more normal value would be 512.\\n. I will then split the training text into sequences of [max_sequence_length + 1]. To make sure the\\n. starting tokens are also considered the data will be padded on the left.Then we are going to train the model for 50 epochs, with a batch size of 8. An epoch means a\\n. complete pass over the training data. Batch size means that in every forward pass through the\\n. model we consider 8 sequences from the training data simultaneously. The higher the batch\\n. size the better the model can learn patterns in the data, but a higher batch size also leads to\\n. more memory usage.\\n. def create_training_sequences(max_sequence_length, tokenized_training_data):\\n. # Create sequences of length max_sequence_length + 1\\n. # The last token of each sequence is the target token\\n. sequences = []\\n. for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\\n. sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\\n. return sequences\\n. def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\\n. # Tokenize the training data\\n. tokenized_training_data = tokenizer.tokenize(training_data)\\n. for _ in range(max_sequence_length):\\n. # Prepend padding tokens\\n. tokenized_training_data.insert(0, tokenizer.character_to_token(\\'<pad>\\'))\\n. return tokenized_training_data\\n. tokenizer = Tokenizer()\\n. embedding_dimension = 256\\n. max_sequence_length = 20\\n. number_of_tokens = tokenizer.size()\\n. # Create the model\\n. model = AutoregressiveWrapper(LanguageModel(\\n. embedding_dimension=embedding_dimension,\\n. number_of_tokens=number_of_tokens,\\n. number_of_heads=4,\\n. number_of_layers=3,\\n. dropout_rate=0.1,\\n. max_sequence_length=max_sequence_length\\n. ))\\n. # Create the training data\\n. training_data = \\'. \\'.join([\\n. \\'cats rule the world\\',\\n. \\'dogs are the best\\',\\'elephants have long trunks\\',\\n. \\'monkeys like bananas\\',\\n. \\'pandas eat bamboo\\',\\n. \\'tigers are dangerous\\',\\n. \\'zebras have stripes\\',\\n. \\'lions are the kings of the savannah\\',\\n. \\'giraffes have long necks\\',\\n. \\'hippos are big and scary\\',\\n. \\'rhinos have horns\\',\\n. \\'penguins live in the arctic\\',\\n. \\'polar bears are white\\'\\n. ])\\n. tokenized_and_padded_training_data =\\n. tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\\n. sequences = create_training_sequences(max_sequence_length,\\n. tokenized_and_padded_training_data)\\n. # Train the model\\n. optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\\n. trainer = Trainer(model, tokenizer, optimizer)\\n. trainer.train(sequences, epochs=100, batch_size=8)\\n. The trainer is a helper class that loops over the epochs and shuffles the data at the start of each\\n. epoch. The reason to do this is to prevent the batches from being the same every time, causing\\n. the model to overfit to these specific batches.\\n. While creating batches we also determine the mask. All padding tokens are masked, meaning\\n. they will not be considered in the attention step.\\n. Then we do a forward pass through the model with a batch. This means we let the model make\\n. predictions using the given data. The predictions are then compared to a target value, which is\\n. the sequence shifted by 1 step so the next token becomes visible. The model outputs\\n. probabilities for what token should be the next. The loss function knows what the answer should\\n. be. The further from its target the prediction was the higher the loss value will be.\\n. When the loss value is calculated the model can be updated. This is done by calculating\\n. gradients; the direction the weights should be adjusted to improve the prediction of the model.\\n. The model is then slightly adjusted in the direction of the gradients, and a new batch can be\\n. processed.\\n. If everything works as planned, the loss should go down over time. I return the loss per epoch,\\n. so it can be plotted.\\n. class Trainer:def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\\n. super().__init__()\\n. self.model = model\\n. if optimizer is None:\\n. self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\\n. else:\\n. self.optimizer = optimizer\\n. self.tokenizer = tokenizer\\n. self.loss_function = torch.nn.CrossEntropyLoss()\\n. def train(self, data: List[str], epochs, batch_size):\\n. loss_per_epoch = []\\n. for epoch in range(epochs):\\n. losses = []\\n. # Shuffle the sequences\\n. random.shuffle(data)\\n. # Create batches of sequences and their respective mask.\\n. batches = []\\n. for i in range(0, len(data), batch_size):\\n. sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\\n. # Create the mask tensor for the batch, where 1 means the token is not a padding\\n. token\\n. mask_tensor = torch.ones_like(sequence_tensor)\\n. mask_tensor[sequence_tensor == self.tokenizer.character_to_token(\\'<pad>\\')] = 0\\n. batches.append((sequence_tensor, mask_tensor))\\n. # Train the model on each batch\\n. for batch in batches:\\n. self.model.train()\\n. # Create the input and mask tensors\\n. input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1),\\n. dtype=torch.long)\\n. mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1),\\n. dtype=torch.long)\\n. for i, input_entry in enumerate(batch[0]):\\n. input_tensor[i] = input_entryfor i, mask_entry in enumerate(batch[1]):\\n. mask_tensor[i] = mask_entry\\n. # Compute the model output\\n. model_output, target = self.model.forward(x=input_tensor, mask=mask_tensor)\\n. # Compute the losses\\n. # The loss is computed on the model output and the target\\n. loss = self.loss_function(model_output.transpose(1, 2), target)\\n. # Backpropagate the loss.\\n. loss.backward()\\n. # Clip the gradients. This is used to prevent exploding gradients.\\n. torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\\n. # Update the model parameters. This is done by taking a step in the direction of the\\n. gradient.\\n. self.optimizer.step()\\n. # Reset the gradients. This is done so that the gradients from the previous batch\\n. # are not used in the next step.\\n. self.optimizer.zero_grad()\\n. # Append the loss to the list of losses, so that the average loss can be computed for\\n. this epoch.\\n. losses.append(loss.item())\\n. # Print the loss\\n. epoch_loss = np.average(losses)\\n. loss_per_epoch.append(epoch_loss)\\n. print(\\'Epoch:\\', epoch, \\'Loss:\\', epoch_loss)\\n. return loss_per_epoch\\n. Evenutally plotting the loss should give us a nice graph with decreasing loss. It is plotted in log\\n. scale, so you can see the smaller variations towards the end of training.\\n. # Plot the loss per epoch in log scale\\n. plt.plot(loss_per_epoch)\\n. plt.yscale(\\'log\\')\\n. plt.ylabel(\\'Loss\\')\\n. plt.xlabel(\\'Epoch\\')\\n. plt.show()\\n. GeneratorNow the model is trained and I want to see if it actually learned to write. Let\\'s try to generate a\\n. text based on the prompt \"elephants\". I want it to continue writing for 50 tokens.\\n. Since the only mention of elephants in the training data is \"elephants have long trunks\", I expect\\n. the model to write this.\\n. max_tokens_to_generate = 50\\n. generator = Generator(model, tokenizer)\\n. generated_text = generator.generate(\\n. max_tokens_to_generate=max_tokens_to_generate,\\n. prompt=\"elephants\",\\n. padding_token=tokenizer.character_to_token(\\'<pad>\\')\\n. )\\n. print(generated_text.replace(\\'<pad>\\', \\'\\'))\\n. but first we need to write the code for the Generator. A helper class for generating text.\\n. First we switch the model from \"training\" mode to \"eval\" mode. In the eval mode the model will\\n. not apply dropout.\\n. The prompt we give is converted to tokens, and then padded so it has the correct sequence\\n. length.\\n. Then we are going to auto-regressively generate new tokens and add them to the input\\n. sequence. After a token is added we run the new input sequence with the extra token through\\n. the model again, and we append a new token. We continue this process until the maximum\\n. number of characters we wanted to generate is reached, or until we have generated the\\n. eos_token, or end of sequence token. This is a token that can be defined by the user as an\\n. indication that we need to stop generating.\\n. def pad_left(sequence, final_length, padding_token):\\n. return [padding_token] * (final_length - len(sequence)) + sequence\\n. class Generator:\\n. def __init__(\\n. self,\\n. model,\\n. tokenizer):\\n. self.model = model\\n. self.tokenizer = tokenizer\\n. def generate(\\n. self,max_tokens_to_generate: int,\\n. prompt: str = None,\\n. temperature: float = 1.0,\\n. eos_token: int = None,\\n. padding_token: int = 0):\\n. self.model.eval()\\n. if prompt is None:\\n. start_tokens = [self.tokenizer.character_to_token(padding_token)]\\n. else:\\n. start_tokens = self.tokenizer.tokenize(prompt)\\n. input_tensor = torch.tensor(\\n. pad_left(\\n. sequence=start_tokens,\\n. final_length=self.model.max_sequence_length + 1,\\n. padding_token=padding_token\\n. ),\\n. dtype=torch.long\\n. )\\n. num_dims = len(input_tensor.shape)\\n. if num_dims == 1:\\n. input_tensor = input_tensor[None, :]\\n. out = input_tensor\\n. for _ in range(max_tokens_to_generate):\\n. x = out[:, -self.model.max_sequence_length:]\\n. mask = torch.ones_like(x)\\n. mask[x == padding_token] = 0\\n. # Compute the next token probabilities\\n. next_token_probabilities = self.model.next_token_probabilities(\\n. x=x,\\n. temperature=temperature,\\n. mask=mask\\n. )\\n. # Sample the next token from the probability distribution\\n. next_token = torch.multinomial(next_token_probabilities, num_samples=1)# Append the next token to the output\\n. out = torch.cat([out, next_token], dim=1)\\n. # If the end of sequence token is reached, stop generating tokens\\n. if eos_token is not None and next_token == eos_token:\\n. break\\n. generated_tokens = out[0].tolist()\\n. return \\'\\'.join([self.tokenizer.token_to_character(token) for token in generated_tokens])\\n. The training is finished and the generator has run. aaaaaanndd... the model actually outputs the\\n. text we trained it on!\\n. Of course, when training on such a small dataset the model will completely overfit it and learn to\\n. reproduce the whole dataset. Not what you are usually looking for in a language model, but in\\n. this \"hello world\" test it means success!\\n. Saving and loading the model\\n. Once you trained the model, it is useful if you can save it, so you don\\'t have to train a new\\n. model every time.\\n. To do this we add the following code to the LanguageModel class.\\n. def save_checkpoint(self, path):\\n. print(f\\'Saving checkpoint {path}\\')\\n. torch.save({\\n. \\'number_of_tokens\\': self.number_of_tokens,\\n. \\'max_sequence_length\\': self.max_sequence_length,\\n. \\'embedding_dimension\\': self.embedding_dimension,\\n. \\'number_of_layers\\': self.number_of_layers,\\n. \\'number_of_heads\\': self.number_of_heads,\\n. \\'feed_forward_dimension\\': self.feed_forward_dimension,\\n. \\'dropout_rate\\': self.dropout_rate,\\n. \\'model_state_dict\\': self.state_dict()\\n. }, path)\\n. @staticmethod\\n. def load_checkpoint(path) -> \\'LanguageModel\\':\\n. checkpoint = torch.load(path)\\n. model = LanguageModel(\\n. number_of_tokens=checkpoint[\\'number_of_tokens\\'],\\n. max_sequence_length=checkpoint[\\'max_sequence_length\\'],\\n. embedding_dimension=checkpoint[\\'embedding_dimension\\'],number_of_layers=checkpoint[\\'number_of_layers\\'],\\n. number_of_heads=checkpoint[\\'number_of_heads\\'],\\n. feed_forward_dimension=checkpoint[\\'feed_forward_dimension\\'],\\n. dropout_rate=checkpoint[\\'dropout_rate\\']\\n. )\\n. model.load_state_dict(checkpoint[\\'model_state_dict\\'])\\n. return model\\n. Since we use the AutoregressiveWrapper as convenience class, we can give this wrapper the\\n. save and load methods too.\\n. def save_checkpoint(self, path):\\n. self.model.save_checkpoint(path)\\n. @staticmethod\\n. def load_checkpoint(path) -> \\'AutoregressiveWrapper\\':\\n. model = LanguageModel.load_checkpoint(path)\\n. return AutoregressiveWrapper(model)\\n. This makes it possible to easily save and load a trained model using.\\n. model.save_checkpoint(\\'./trained_model\\')\\n. model = model.load_checkpoint(\\'./trained_model\\')\\n. Running on GPU\\n. If you have a GPU at your disposal and Cuda is configured, you can use a GPU to speed up\\n. training.\\n. First we need to define a function to determine if we can use a GPU:\\n. def get_device():\\n. if torch.cuda.is_available():\\n. return torch.device(\\'cuda\\')\\n. else:\\n. return torch.device(\\'cpu\\')\\n. Then we need to move the model, and all the input tensors to our device. We can do this by\\n. using the torch function \"to\".\\n. # Model\\n. model = AutoregressiveWrapper(LanguageModel(\\n. embedding_dimension=embedding_dimension,\\n. number_of_tokens=number_of_tokens,\\n. number_of_heads=4,\\n. number_of_layers=3,\\n. dropout_rate=0.1,\\n. max_sequence_length=max_sequence_length)).to(get_device())\\n. # Training input\\n. model_output, target = self.model.forward(\\n. x=input_tensor.to(get_device()),\\n. mask=mask_tensor.to(get_device())\\n. )\\n. # Generation input\\n. input_tensor = torch.tensor(\\n. pad_left(\\n. sequence=start_tokens,\\n. final_length=self.model.max_sequence_length + 1,\\n. padding_token=padding_token\\n. ),\\n. dtype=torch.long\\n. ).to(get_device())\\n. The speedup achieved by using a GPU can be really significant.\\n. Conclusion\\n. While writing this blogpost I learned a lot about how the transformer works, and how we can use\\n. it to generate text. I hope you enjoyed reading it, and learned something new too!\\n. Feel free to leave a comment if you have remarks, questions or just want to let me know what\\n. you think :)\\n. The code for this blogpost is available at https://github.com/wingedsheep/transformer\\n. Sources\\n. Some resources that were incredibly helpful in helping me understand attention, the transformer\\n. architecture and GPT models.\\n. Attention is all you need by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\n. Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin https://arxiv.org/abs/1706.03762\\n. Transformer implementation by Phil Wang (Lucidrains)\\n. https://github.com/lucidrains/x-transformers\\n. The illustrated transformer by Jay Alammar https://jalammar.github.io/illustrated-transformer/\\n. Illustrated GPT-2 by Jay Alammar https://jalammar.github.io/illustrated-gpt2/\\n. MinGPT by Andrej Karpathy https://github.com/karpathy/minGPT\\n. Seq2seq and attention by Lena Voita\\n. https://lena-voita.github.io/nlp_course/seq2seq_and_attention.htmlThe GPT-3 Architecture, on a Napkin by Daniel Dugas\\n. https://dugas.ch/artificial_curiosity/GPT_architecture.html\\n. Wingedsheep: Artificial Intelligence Blog\\n. A blog about artificial intelligence, machine learning, programming and anything else that\\n. interests me.\\n. Read more posts \\n. Published with Ghost'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"makedata/data.txt\", 'r') as file:\n",
    "    training_data = '. '.join([line for line in file.readlines()])\n",
    "\n",
    "training_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white \n"
     ]
    }
   ],
   "source": [
    "max_tokens_to_generate = 50\n",
    "generator = Generator(model, tokenizer)\n",
    "generated_text = generator.generate(\n",
    "max_tokens_to_generate=max_tokens_to_generate,\n",
    "prompt=\"white\",\n",
    "padding_token=tokenizer.character_to_token('<pad>')\n",
    ")\n",
    "print(generated_text.replace('<pad>', ''))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AutoregressiveWrapper' object has no attribute 'save_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     model \u001b[39m=\u001b[39m LanguageModel\u001b[39m.\u001b[39mload_checkpoint(path)\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m AutoregressiveWrapper(model)\n\u001b[0;32m---> 40\u001b[0m model\u001b[39m.\u001b[39;49msave_checkpoint(\u001b[39m'\u001b[39m\u001b[39m./trained_model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mload_checkpoint(\u001b[39m'\u001b[39m\u001b[39m./trained_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoregressiveWrapper' object has no attribute 'save_checkpoint'"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(self, path):\n",
    "    print(f'Saving checkpoint {path}')\n",
    "    torch.save({\n",
    "    'number_of_tokens': self.number_of_tokens,\n",
    "    'max_sequence_length': self.max_sequence_length,\n",
    "    'embedding_dimension': self.embedding_dimension,\n",
    "    'number_of_layers': self.number_of_layers,\n",
    "    'number_of_heads': self.number_of_heads,\n",
    "    'feed_forward_dimension': self.feed_forward_dimension,\n",
    "    'dropout_rate': self.dropout_rate,\n",
    "    'model_state_dict': self.state_dict()\n",
    "    }, path)\n",
    "\n",
    "@staticmethod\n",
    "def load_checkpoint(path) -> 'LanguageModel':\n",
    "    checkpoint = torch.load(path)\n",
    "    model = LanguageModel(\n",
    "    number_of_tokens=checkpoint['number_of_tokens'],\n",
    "    max_sequence_length=checkpoint['max_sequence_length'],\n",
    "    embedding_dimension=checkpoint['embedding_dimension'],number_of_layers=checkpoint['number_of_layers'],\n",
    "    number_of_heads=checkpoint['number_of_heads'],\n",
    "    feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
    "    dropout_rate=checkpoint['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Since we use the AutoregressiveWrapper as convenience class, we can give this wrapper the\n",
    "# save and load methods too.\n",
    "\n",
    "def save_checkpoint(self, path):\n",
    "    self.model.save_checkpoint(path)\n",
    "\n",
    "@staticmethod\n",
    "def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
    "    model = LanguageModel.load_checkpoint(path)\n",
    "    return AutoregressiveWrapper(model)\n",
    "\n",
    "model.save_checkpoint('./trained_model')\n",
    "model = model.load_checkpoint('./trained_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
